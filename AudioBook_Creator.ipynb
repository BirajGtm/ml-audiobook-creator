{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an Audiobook from a PDF\n",
    "## This task tests your ability to apply Text to Speech conversion and Extraction of Text from PDF files in the creation of an audiobook from a PDF file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "- Extract text from PDF file\n",
    "- Clean the text\n",
    "- Convert the text into speech\n",
    "- Save the speech\n",
    "- Play the speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract text from PDF\n",
    "- Use PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "    \n",
    "\n",
    "def clean_pdf_text(raw_text):\n",
    "    cleaned = re.sub(r'-\\n', '', raw_text)\n",
    "    cleaned = re.sub(r'\\n+', ' ', cleaned)\n",
    "    cleaned = re.sub(r'\\s{2,}', ' ', cleaned)\n",
    "    cleaned = re.sub(r'\\b\\d+\\b', '', cleaned)\n",
    "    return cleaned.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention is All You Need . Introduction The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder . The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer , based solely on attention mechanisms, dispensing with recurrence and convolutions entirely . Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves . BLEU on the WMT  English-to-German translation task, improving over the existing best results, including ensembles. On the WMT  English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of . after training for . days on eight GPUs, a small fraction of the training costs of the best models from the literature. . Background The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU, ByteNet, and ConvS2S. Both models use stacked convolutional layers to encode the input and decode the output. In these models, the number of sequential operations required to relate signals between any two input or output positions grows with the distance between positions. More recently , self-attention has been used successfully in various NLP tasks. Our Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. . Model Architecture The overall architecture follows the typical encoder-decoder structure. The encoder maps an input sequence of symbol representations to a sequence of continuous representations. Given z, the decoder then generates an output sequence of symbols one element at a time. At each step, the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next. . Encoder and Decoder Stacks The encoder is composed of a stack of N =  identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a position-wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d_model = . The decoder is also composed of a stack of N =  identical layers. In addition to the two sub-layers in each encoder layer , the decoder inserts a third sub-layer , which performs multi-head attention over the output of the encoder stack. Similar to the encoder , we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i. . Attention An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query , keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. We call our particular attention “Scaled Dot-Product Attention”. The input consists of queries and keys of dimension d_k, and values of dimension d_v. We compute the dot products of the query with all keys, divide each by sqrt(d_k), and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously , packed together into a matrix Q. The keys and values are also packed together into matrices K and V. We compute the matrix of outputs as: Attention(Q, K, V) = softmax(QK^T / sqrt(d_k)) V . Multi-Head Attention Instead of performing a single attention function with d_model-dimensional keys, values, and queries, we found it beneficial to linearly project the queries, keys and values h times with different learned linear projections to d_k, d_k, and d_v dimensions, respectively . On each of these projected versions of queries, keys, and values we then perform the attention function in parallel, yielding d_v-dimensional output values. These are concatenated and once again projected, resulting in the final values: MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W^O where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V) . Position-wise Feed-Forward Networks In addition to attention sub-layers, each layer in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically . This consists of two linear transformations with a ReLU activation in between. FFN(x) = max(, xW_1 + b_1)W_2 + b_2 . Embeddings and Positional Encoding Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension d_model as the embeddings, so that the two can be summed. There are many choices of positional encodings; we use sine and cosine functions of different frequencies: PE(pos, 2i) = sin(pos / ^(2i/d_model)) PE(pos, 2i+) = cos(pos / ^(2i/d_model)) . Why Self-Attention Self-attention layers connect all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) operations. In terms of computation, self-attention layers can be faster and more parallelizable, although they do require more memory for shorter sequences. In practice, self-attention achieves similar or better accuracy on a variety of tasks and allows for significantly more training parallelization. . Training We trained our models on the WMT  English-German and English-French datasets. Sentences were encoded using byte-pair encoding, and we shared vocabulary between the source and target for English-German. Our base model used  layers in both the encoder and decoder , and our big model increased the hidden size and the number of heads. We used dropout with probability . on all layers. . Results On the English-to-German task, our big model achieves a BLEU score of ., outperforming previous benchmarks. On the English-to-French task, our model achieved . BLEU. These results demonstrate the efficacy of the Transformer architecture. . Conclusion In this work, we presented the Transformer , the first sequence transduction model based entirely on attention mechanisms, dispensing with recurrence and convolutions entirely . We show that the Transformer achieves superior results on translation tasks while also enabling significantly faster training. We believe the Transformer represents a new paradigm for many NLP tasks going forward.\n"
     ]
    }
   ],
   "source": [
    "text = extract_text_from_pdf(\"doc.pdf\")\n",
    "clean_text = clean_pdf_text(text)\n",
    "\n",
    "print(clean_text)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert the Text into Speech\n",
    "- Use **pyttsx3** OR **gTTS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gTTS in /opt/anaconda3/envs/scikit/lib/python3.12/site-packages (2.5.4)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/anaconda3/envs/scikit/lib/python3.12/site-packages (from gTTS) (2.32.3)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in /opt/anaconda3/envs/scikit/lib/python3.12/site-packages (from gTTS) (8.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/scikit/lib/python3.12/site-packages (from requests<3,>=2.27->gTTS) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/scikit/lib/python3.12/site-packages (from requests<3,>=2.27->gTTS) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/scikit/lib/python3.12/site-packages (from requests<3,>=2.27->gTTS) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/scikit/lib/python3.12/site-packages (from requests<3,>=2.27->gTTS) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyttsx3\n",
    "# %pip install gTTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize a Speaker object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not required on gtts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_text_to_speech(text, language='en', output_file='audio.mp3'):\n",
    "    tts = gTTS(text=text, lang=language, slow=False)\n",
    "    tts.save(output_file)\n",
    "    print(f\"Audio saved as {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio saved as audio.mp3\n"
     ]
    }
   ],
   "source": [
    "convert_text_to_speech(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scikit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
